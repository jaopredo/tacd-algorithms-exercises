{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4c9d77-8ce0-4600-b5db-f98f39c8e9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163fb6c-8186-42af-885f-8278e7ca2ce3",
   "metadata": {},
   "source": [
    "# Tarefa 4 - Decision Trees, Random Forest and K-Means\n",
    "Fourth assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using decision trees and random forests to solve a real-world problem in classification and delve into the world of unsupervised learning by implementing the K-means algorithm. Students will also get used to generating important plots during training to analyse the models' behaviour. \n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).\n",
    "* Please try to not use any LLM-generated code. This coursework is designed for you to learn crucial concepts. Once you master them, then using LLMs become much easier.\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Decision Trees](#Decision_Trees) (30%)\n",
    "2. [Random Forest](#Random_Forest) (30%)\n",
    "3. [K-Means](#K-Means) (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bb261-de78-4c09-bbff-f1128567f078",
   "metadata": {},
   "source": [
    "### Decision_Trees\n",
    "## Part 1 - Decision Trees for Classification (value: 30%)\n",
    "\n",
    "In this exercise, you will implement a decision tree for classifying whether the income of a person exceeds $50k/yr based on census data (adult_census_subset.csv in ECLASS). You will use the Information Gain based on the Gini Index as the impurity measure as the splitting criterion. The maximum depth and the minimum number of instances per leaf will be your stopping criteria. Be aware that some of the variables in this dataset are nominal (or categorical).\n",
    "\n",
    "To complete this exercise, you will write code to build a decision tree for this problem: \n",
    "\n",
    "1. Dataset Splitting:\n",
    "    - Load the provided dataset into your code.\n",
    "\t- Split the dataset into three sets: training, validation, and testing, with a 70/15/15 ratio, respectively. \n",
    "2. Implement a function to learn Decision Trees – the main conceptual steps are detailed below:\n",
    "\t- Initialize an empty decision tree.\n",
    "\t- Implement a recursive function to build the decision tree:\n",
    "        - The stopping conditions for the recursive function are [note: satisfying only one of them is sufficient to stop the recursion]:\n",
    "            - If the maximum depth is reached, stop growing the tree and create a leaf node with the frequency of the positive class for the remaining instances.\n",
    "            - If the number of instances at a leaf node is less than the minimum number of instances per leaf, create a leaf node with the frequency of the positive class for those instances.\n",
    "        - Your code will calculate the Information Gain (based on the Gini Index) for each possible value of each attribute and choose the attribute and value that maximizes the Information Gain (explanation below).\n",
    "        - Your code will create a new internal node using the chosen attribute and value.\n",
    "        - Your code will recursively call the build function on each subset of instances created by the split.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the decision tree:\n",
    "\t- For each instance, traverse the decision tree by comparing its attribute values to the decision nodes and move down the tree based on the attribute values until a leaf node is reached.\n",
    "\t- Return the frequency of the positive class that is associated with the leaf node as the prediction for the instance.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the build function with the training set to construct the decision tree. You will vary the maximum depth and minimum number of instances per leaf to observe their effects on the decision tree performance. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Try only shallow trees of a depth not deeper than 10, and min_instances not smaller than 10. If you try more extreme values, the training time could be too much.\n",
    "\t- Build a decision tree using the training + validation sets with the best combination of parameters.\n",
    "\t- Calculate the accuracy (threshold: 0.5) and AUROC of the decision tree in the testing set and report them.\n",
    "\n",
    "To select the best split at each node you will use the Information Gain based on the Gini Index. The Gini Index measures the impurity of a node in a decision tree. To calculate the Information Gain based on the Gini Index, follow these steps [note: the same is explained in the slides for the case of entropy]:\n",
    "- For each potential split (feature and value):\n",
    "\t- Calculate the Gini Index for node m (before any splits) using the class distribution within the node, using the following formula:\n",
    "        - $G_m=\\sum_{k=1}^K (\\hat{p}_{mk} (1-\\hat{p}_{mk})$, where $\\hat{p}_{mk}$ represents the proportion of instances in the node $m$ that belong to class $k$.\n",
    "\t- Calculate the Gini Index for each possible outcome. This involves the following steps:\n",
    "        - Split the data based on the attribute's possible outcomes.\n",
    "        - Calculate the Gini index for each resulting subset using the same formula as in step a.\n",
    "\t- Calculate the weighted Gini index by summing up the Gini indexes of each subset, weighted by the proportion of instances it represents in the original node. The formula for the weighted Gini index ($W$) is as follows:\n",
    "        - $W_V=\\sum_v^V \\frac{|S_v|}{|S|} G_{S_v} $ where $S_v$ is the node after the split and the sum iterates over all the children nodes; $|S_v|$ represents the cardinality of the node and $|S|$ the cardinality of the node before splitting; $G_{S_v}$ represents the Gini index of the node.\n",
    "\t- Calculate the information gain by subtracting the weighted Gini index obtained in step c. from the Gini index of the current node. The formula is as follows:\n",
    "        - $InformationGain=G_{node}-W_V$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51329343-9ecb-4e89-8b7b-bdfd62c4d777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading my dataset\n",
    "adult_census = pd.read_csv(\"adult_census_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f353fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating dataset and target\n",
    "X = adult_census.drop(\"income\", axis=1)\n",
    "t = adult_census[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f10489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "t = data.target\n",
    "\n",
    "X = X[t != 2]\n",
    "t = t[t != 2]\n",
    "\n",
    "X = pd.DataFrame(X, columns=[i for i in range(X.shape[1])])\n",
    "t = pd.Series(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25460fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating my subsets\n",
    "X_train, X_split, t_train, t_split = train_test_split(X, t, train_size=.7)\n",
    "X_validation, X_test, t_validation, t_test = train_test_split(X_split, t_split, train_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddf9e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=6, min_samples=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, t: pd.Series):\n",
    "        \"\"\"This function fits my decision tree on the dataset and target passed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            The dataset that will be fitted onto the tree\n",
    "        t : pd.Series\n",
    "            The target for reach datapoint of the passed dataset\n",
    "        \"\"\"\n",
    "        self.root = self.get_split(X, t, self.gini_index(t))\n",
    "        self.root = self.recursive_growth(self.root, 0, X, t)\n",
    "    \n",
    "    def recursive_growth(self, node: dict, current_depth: int, X: pd.DataFrame, t: pd.Series):\n",
    "        \"\"\"\n",
    "        Recursively grows a decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dict\n",
    "            If the node is terminal, it contains only the \"value\" property, which determines the value\n",
    "            to be used as a prediction.\n",
    "            If the node is not terminal, the Node has the structure defined by `get_split`\n",
    "        min_samples : int\n",
    "            parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "        max_depth : int\n",
    "            parameter for stopping criterion if a node belongs to this depth\n",
    "        depth : int\n",
    "            current distance from the root\n",
    "        X : pd.DataFrame\n",
    "            features (full dataset)\n",
    "        t : pd.Series\n",
    "            labels, targets (full dataset)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "        is the mean of the target variable\n",
    "        \n",
    "        'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "        from higher levels of the tree:\n",
    "        'left' corresponds to the 'left_region' key, and 'right' to the 'right_region' key\n",
    "        \"\"\"\n",
    "        l_idx = node['left_region']\n",
    "        r_idx = node['right_region']\n",
    "\n",
    "        if                                                                                  \\\n",
    "            (len(X) <= self.min_samples)             \\\n",
    "            or                                                                              \\\n",
    "            (current_depth >= self.max_depth)                                                    \\\n",
    "        :\n",
    "            return {\n",
    "                \"value\": np.mean(t)\n",
    "            }\n",
    "        \n",
    "        # X = X.drop(node['feature_index'], axis=1)\n",
    "        splited_infos = self.get_split(X, t, node['gini_index'])\n",
    "        l_idx = splited_infos['left_region']\n",
    "        r_idx = splited_infos['right_region']\n",
    "        \n",
    "        return {\n",
    "            'feature_index': splited_infos['feature_index'],\n",
    "            'tau': splited_infos['tau'],\n",
    "            'left': self.recursive_growth(splited_infos, current_depth+1, X.loc[l_idx], t.loc[l_idx]),\n",
    "            'right': self.recursive_growth(splited_infos, current_depth+1, X.loc[r_idx], t.loc[r_idx])\n",
    "        }\n",
    "    \n",
    "    def get_split(self, X: pd.DataFrame, y: pd.Series, gini_index: float):\n",
    "        \"\"\"\n",
    "        Given a dataset (full or partial), splits it on the feature of that minimizes the\n",
    "        sum of squared error\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            features \n",
    "        y : pd.Series\n",
    "            labels\n",
    "        gini_index: float\n",
    "            This function is used \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        decision : dictionary\n",
    "            keys are:\n",
    "            * 'feature_index' -> an integer that indicates the feature (column) of `X`\n",
    "            on which the data is split\n",
    "            * 'tau' -> the threshold used to make the split\n",
    "            * 'left_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "            * 'right_region' -> indices not in `low_region`\n",
    "        \"\"\"\n",
    "        best_tau_separations = []\n",
    "        # List containing the best tau and gini_index registered for each feature, follows this structure\n",
    "\n",
    "        for feature in X.columns:  # Going through every feature\n",
    "            feature_information_gains = []\n",
    "            # List containing all registered SSE for this features (It follows this structure:)\n",
    "            # [ (tau_1, sse_1), (tau_2, sse_2), ..., (tau_j, sse_j) ]\n",
    "            # Where 'tau_i' is the ith element of the feature's column and 'sse_i' is its registered SSE\n",
    "            unique_features = pd.unique(X[feature])\n",
    "\n",
    "            for tau in unique_features:  # Using each value of the feature as tau (For performance reasons)\n",
    "                # First I get the node division based on the tau and the feature specified\n",
    "                l_feature, r_feature = self.split_region(X, feature, tau)\n",
    "                # I initialize my weighted gini index\n",
    "                tau_weighted_gini_index = 0\n",
    "\n",
    "                # I store the divisions on variables for organization purposes\n",
    "                y_right = y[r_feature]\n",
    "                y_left = y[l_feature]\n",
    "\n",
    "                # I get the gini index for the left node and the right node\n",
    "                gini_left = self.gini_index(y_left)\n",
    "                gini_right = self.gini_index(y_right)\n",
    "\n",
    "                # For organization purposes I store the amount of elements in the parent node, its\n",
    "                # left division and right division\n",
    "                amount_total = len(y)\n",
    "                amount_right = len(y_right)\n",
    "                amount_left = len(y_left)\n",
    "\n",
    "                # Now I sum up the weighted gini index as the definition says\n",
    "                tau_weighted_gini_index += (amount_left/amount_total)*gini_left\n",
    "                tau_weighted_gini_index += (amount_right/amount_total)*gini_right\n",
    "\n",
    "                # Now I calculate the information gain for this specific feature and tau\n",
    "                tau_information_gain = gini_index - tau_weighted_gini_index\n",
    "                \n",
    "                feature_information_gains.append((tau, tau_information_gain, gini_left + gini_right))\n",
    "\n",
    "            maximum_tau_information_gain = max(feature_information_gains, key=lambda f_s: f_s[1])\n",
    "            # Getting the maximum information gain\n",
    "            best_tau_separations.append((feature, *maximum_tau_information_gain))\n",
    "            # Store the best tau separation in a list for later get the best option\n",
    "        \n",
    "        best_separation_criterion = max(best_tau_separations, key=lambda s_c: s_c[2])\n",
    "        # Now I get the best information gain based on the tau and the feature\n",
    "\n",
    "        l, r = self.split_region(X, best_separation_criterion[0], best_separation_criterion[1])\n",
    "\n",
    "        return {\n",
    "            'feature_index': best_separation_criterion[0],\n",
    "            'tau': best_separation_criterion[1],\n",
    "            'left_region': l,\n",
    "            'right_region': r,\n",
    "            'gini_index': best_separation_criterion[3],\n",
    "            'information_gain': best_separation_criterion[2]\n",
    "        }\n",
    "    \n",
    "    def split_region(self, region: pd.Series | pd.DataFrame, feature_index: int, tau: float):\n",
    "        \"\"\"\n",
    "        Given a region, splits it based on the feature indicated by\n",
    "        `feature_index`, the region will be split in two, where\n",
    "        one side will contain all points with the feature with values \n",
    "        lower than `tau`, and the other split will contain the \n",
    "        remaining datapoints.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : pd.Series | pd.DataFrame\n",
    "            a partition of the dataset (or the full dataset) to be split\n",
    "        feature_index : int\n",
    "            the index of the feature (column of the region array) used to make this partition\n",
    "        tau : float\n",
    "            The threshold used to make this partition\n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        left_partition : pd.Series | pd.DataFrame\n",
    "            indices of the datapoints in `region` where feature < `tau`\n",
    "        right_partition : pd.Series | pd.DataFrame\n",
    "            indices of the datapoints in `region` where feature >= `tau` \n",
    "        \"\"\"\n",
    "        left_partition = region[region[feature_index] < tau][feature_index].index\n",
    "        right_partition = region[region[feature_index] >= tau][feature_index].index\n",
    "\n",
    "        return left_partition, right_partition\n",
    "    \n",
    "    def gini_index(self, region: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Implements the gini index criterion in a region\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : pd.Series\n",
    "            Array of shape (N,) containing the values of the target values \n",
    "            for N datapoints in the training set.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The sum of squared error\n",
    "            \n",
    "        Note\n",
    "        ----\n",
    "        The error for an empty region should be infinity (use: float(\"inf\"))\n",
    "        This avoids creating empty regions\n",
    "        \"\"\"\n",
    "        if len(region) == 0:\n",
    "            return 0\n",
    "        if isinstance(region, pd.Series):\n",
    "            region = region.to_numpy()\n",
    "        \n",
    "        classes = np.unique(region)\n",
    "        Gm = 0\n",
    "        \n",
    "        for k in classes:\n",
    "            p_mk = len(region[region == k]) / len(region)\n",
    "            Gm += p_mk * (1 - p_mk)\n",
    "\n",
    "        return Gm\n",
    "    \n",
    "    def predict_sample(self, sample: pd.Series, node: dict):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dictionary\n",
    "            A node created one of the methods above\n",
    "        sample : array of size (n_features,)\n",
    "            a sample datapoint\n",
    "        \"\"\"\n",
    "        if 'value' in node.keys():\n",
    "            return node['value']\n",
    "        else:\n",
    "            if sample[node['feature_index']] < node['tau']:\n",
    "                return self.predict_sample(sample, node['left'])\n",
    "            else:\n",
    "                return self.predict_sample(sample, node['right'])\n",
    "\n",
    "    def predict(self, X,):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of size (n_samples, n_features)\n",
    "            n_samples predictions will be made\n",
    "        \"\"\"\n",
    "        predicted_values = pd.Series(np.array([x for x in range(len(X))]))\n",
    "        predicted_values = predicted_values.apply(lambda i: self.predict_sample(X.iloc[i], self.root))\n",
    "        return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "142e8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0fb7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(t: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"Gets the precision using the passed parameters\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "    t: pd.Series\n",
    "        True labels\n",
    "    y: pd.Series\n",
    "        Predictions\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    float: The precision of the given prediction\n",
    "    \"\"\"\n",
    "    y = y.to_numpy()\n",
    "    t = t.to_numpy()\n",
    "    TP = len(y[\n",
    "        (y == 1) & (t == 1  )\n",
    "    ])\n",
    "    FP = len(y[\n",
    "        (y == 1) & (t == 0)\n",
    "    ])\n",
    "    if TP + FP == 0:\n",
    "        return 0\n",
    "    precision = TP / (TP + FP)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f101ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth):\n",
    "    if 'value' in node.keys():\n",
    "        print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "    else:\n",
    "        print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b128f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = tree.predict(X_test)\n",
    "\n",
    "get_precision(t_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a1622-54da-4aa8-a1c2-00f3241292be",
   "metadata": {},
   "source": [
    "## Random_Forest\n",
    "## Part 2 - Random Forest for Classification Networks (value: 30%)\n",
    "\n",
    "In this exercise, you will expand on the previous exercise and implement Random Forests. You will build an ensemble of decision trees and use them for the same classification task from Part 1. The dataset used for this exercise will be the same as in the previous exercise. Your task is to write code to construct a Random Forest model, evaluate its performance, and compare it to the decision tree implementation. \n",
    "\n",
    "To complete this exercise, you will write code to implement Random Forest for this problem: \n",
    "\n",
    "1. Dataset Splitting: use the same splits you used for Part 1.\n",
    "2. Implement a function to learn Random Forest – the main steps are detailed below:\n",
    "\t- Initialize an empty Random Forest.\n",
    "\t- Determine the number of decision trees to include in the forest (e.g. 20), and the number of the random features to consider, generally `num_features` $≈\\sqrt{p}$ where $p$ is the total number of features.\n",
    "\t- Implement a loop to build the specified number of decision trees:\n",
    "        - Generate a bootstrap sample from the training set (sampling with replacement).\n",
    "        - Build a decision tree using the bootstrap sample, using your implementation from Part I.\n",
    "        - Add the constructed decision tree to the Random Forest.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the Random Forest:\n",
    "\t- For each instance, pass it through all decision trees in the Random Forest and collect the predictions. Note that you should binarize the prediction of each decision tree, that is, use a threshold of 0.5 to determine the actual class label.\n",
    "\t- The prediction for the random forest will be the frequency of the positive class in the predictions collected by all the decision trees.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the function to learn the Random Forest with your training set. You will vary the different parameters of the Random Forest to observe their effect on the performance on the validation set. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Again, keep your trees shallow and don’t build many decision trees, as this could delay the training time quite a lot.\n",
    "\t- Build a Random Forest using the training + validation sets with the best combination of parameters.\n",
    "\t- Classify the instances of the testing set using the Random Forest, calculate the accuracy (threshold: 0.5) and Area Under the ROC Curve (AUROC) and report the results.\n",
    "5. Experimentation: Compare the performance of Random Forests with the single decision tree implementation from the previous exercise reporting the performance on the test set in a table (either a dataframe or markdown). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae13d9f-4ef0-4735-8190-1bcaaa28ea2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2950e57-8657-4066-b657-1c96c830ce6e",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "## Part 3 – Clustering with K-means (value: 40%)\n",
    "\n",
    "In this exercise, you will explore clustering by implementing the K-means algorithm. You will write code to perform K-means clustering while visualizing the movement of the centroids at each iteration. \n",
    "\n",
    "To complete this exercise, you will write code to implement K-means for clustering: \n",
    "\n",
    "1. Dataset Preparation: Run the cells provided in the notebook that generate the artificial data points for this exercise.\n",
    "2. K-means Clustering:\n",
    "\t- Initialize K cluster centroids by selecting K points from your dataset at random.\n",
    "\t- Implement a loop to perform the following steps until convergence (or until a specified maximum number of iterations is reached, e.g., 150):\n",
    "        - Assign each data point to the nearest centroid (you will have to calculate the Euclidean distance between the data point and each centroid).\n",
    "        - Update each centroid by moving it to the mean of all data points assigned to it.\n",
    "        - Check for convergence by comparing the new centroids with the previous centroids. If the difference is smaller than an $\\epsilon=1^{-4}$, exit the loop.\n",
    "3. Centroid Movement Visualization:\n",
    "\t- At 5 different moments during training, plot a figure showing the centroids and the points. Figure 1 should show the situation at the beginning, before learning. Figure 5 should show the situation at the end of the learning. The remaining Figures 2-4 should show intermediary situations.\n",
    "\t- For each figure, each centroid will be represented by a large black cross and each cluster with a different colour, the points must be coloured according to their respective cluster.\n",
    "4. Sum of squared distances:\n",
    "\t- Along with plotting the centroid movement, calculate the sum of squared distances at each iteration as follows:\n",
    "        - $\\sum_{j=1}^K \\sum_{n \\in S_j}d(x_n,\\mu_j )^2$, where $K$ is the number of clusters, $x_n$ represents the $n^{th}$ datapoint, $n \\in S_j$ indicates a set of points that belong to cluster $S_j$, $\\mu_j$ is the mean of the datapoints in $S_j$ and $d(x_n,\\mu_j)$ indicates the Euclidean distance between $x_n$ and $\\mu_j$.\n",
    "\t- Make a plot of the sum of squared distances at each iteration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03418cb-b1b2-4fd8-b0e9-6d1a21ae5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data points\n",
    "np.random.seed(13)\n",
    "num_samples = 200\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features) * 1.5 + np.array([[2, 2]])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 3 + np.array([[-5, -5]])])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 2 + np.array([[7, -5]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b782b9-0bd4-4085-a5f2-f55ca13208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
