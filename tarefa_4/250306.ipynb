{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4c9d77-8ce0-4600-b5db-f98f39c8e9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163fb6c-8186-42af-885f-8278e7ca2ce3",
   "metadata": {},
   "source": [
    "# Tarefa 4 - Decision Trees, Random Forest and K-Means\n",
    "Fourth assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using decision trees and random forests to solve a real-world problem in classification and delve into the world of unsupervised learning by implementing the K-means algorithm. Students will also get used to generating important plots during training to analyse the models' behaviour. \n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).\n",
    "* Please try to not use any LLM-generated code. This coursework is designed for you to learn crucial concepts. Once you master them, then using LLMs become much easier.\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Decision Trees](#Decision_Trees) (30%)\n",
    "2. [Random Forest](#Random_Forest) (30%)\n",
    "3. [K-Means](#K-Means) (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bb261-de78-4c09-bbff-f1128567f078",
   "metadata": {},
   "source": [
    "### Decision_Trees\n",
    "## Part 1 - Decision Trees for Classification (value: 30%)\n",
    "\n",
    "In this exercise, you will implement a decision tree for classifying whether the income of a person exceeds $50k/yr based on census data (adult_census_subset.csv in ECLASS). You will use the Information Gain based on the Gini Index as the impurity measure as the splitting criterion. The maximum depth and the minimum number of instances per leaf will be your stopping criteria. Be aware that some of the variables in this dataset are nominal (or categorical).\n",
    "\n",
    "To complete this exercise, you will write code to build a decision tree for this problem: \n",
    "\n",
    "1. Dataset Splitting:\n",
    "    - Load the provided dataset into your code.\n",
    "\t- Split the dataset into three sets: training, validation, and testing, with a 70/15/15 ratio, respectively. \n",
    "2. Implement a function to learn Decision Trees – the main conceptual steps are detailed below:\n",
    "\t- Initialize an empty decision tree.\n",
    "\t- Implement a recursive function to build the decision tree:\n",
    "        - The stopping conditions for the recursive function are [note: satisfying only one of them is sufficient to stop the recursion]:\n",
    "            - If the maximum depth is reached, stop growing the tree and create a leaf node with the frequency of the positive class for the remaining instances.\n",
    "            - If the number of instances at a leaf node is less than the minimum number of instances per leaf, create a leaf node with the frequency of the positive class for those instances.\n",
    "        - Your code will calculate the Information Gain (based on the Gini Index) for each possible value of each attribute and choose the attribute and value that maximizes the Information Gain (explanation below).\n",
    "        - Your code will create a new internal node using the chosen attribute and value.\n",
    "        - Your code will recursively call the build function on each subset of instances created by the split.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the decision tree:\n",
    "\t- For each instance, traverse the decision tree by comparing its attribute values to the decision nodes and move down the tree based on the attribute values until a leaf node is reached.\n",
    "\t- Return the frequency of the positive class that is associated with the leaf node as the prediction for the instance.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the build function with the training set to construct the decision tree. You will vary the maximum depth and minimum number of instances per leaf to observe their effects on the decision tree performance. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Try only shallow trees of a depth not deeper than 10, and min_instances not smaller than 10. If you try more extreme values, the training time could be too much.\n",
    "\t- Build a decision tree using the training + validation sets with the best combination of parameters.\n",
    "\t- Calculate the accuracy (threshold: 0.5) and AUROC of the decision tree in the testing set and report them.\n",
    "\n",
    "To select the best split at each node you will use the Information Gain based on the Gini Index. The Gini Index measures the impurity of a node in a decision tree. To calculate the Information Gain based on the Gini Index, follow these steps [note: the same is explained in the slides for the case of entropy]:\n",
    "- For each potential split (feature and value):\n",
    "\t- Calculate the Gini Index for node m (before any splits) using the class distribution within the node, using the following formula:\n",
    "        - $G_m=\\sum_{k=1}^K (\\hat{p}_{mk} (1-\\hat{p}_{mk})$, where $\\hat{p}_{mk}$ represents the proportion of instances in the node $m$ that belong to class $k$.\n",
    "\t- Calculate the Gini Index for each possible outcome. This involves the following steps:\n",
    "        - Split the data based on the attribute's possible outcomes.\n",
    "        - Calculate the Gini index for each resulting subset using the same formula as in step a.\n",
    "\t- Calculate the weighted Gini index by summing up the Gini indexes of each subset, weighted by the proportion of instances it represents in the original node. The formula for the weighted Gini index ($W$) is as follows:\n",
    "        - $W_V=\\sum_v^V \\frac{|S_v|}{|S|} G_{S_v} $ where $S_v$ is the node after the split and the sum iterates over all the children nodes; $|S_v|$ represents the cardinality of the node and $|S|$ the cardinality of the node before splitting; $G_{S_v}$ represents the Gini index of the node.\n",
    "\t- Calculate the information gain by subtracting the weighted Gini index obtained in step c. from the Gini index of the current node. The formula is as follows:\n",
    "        - $InformationGain=G_{node}-W_V$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51329343-9ecb-4e89-8b7b-bdfd62c4d777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a1622-54da-4aa8-a1c2-00f3241292be",
   "metadata": {},
   "source": [
    "## Random_Forest\n",
    "## Part 2 - Random Forest for Classification Networks (value: 30%)\n",
    "\n",
    "In this exercise, you will expand on the previous exercise and implement Random Forests. You will build an ensemble of decision trees and use them for the same classification task from Part 1. The dataset used for this exercise will be the same as in the previous exercise. Your task is to write code to construct a Random Forest model, evaluate its performance, and compare it to the decision tree implementation. \n",
    "\n",
    "To complete this exercise, you will write code to implement Random Forest for this problem: \n",
    "\n",
    "1. Dataset Splitting: use the same splits you used for Part 1.\n",
    "2. Implement a function to learn Random Forest – the main steps are detailed below:\n",
    "\t- Initialize an empty Random Forest.\n",
    "\t- Determine the number of decision trees to include in the forest (e.g. 20), and the number of the random features to consider, generally `num_features` $≈\\sqrt{p}$ where $p$ is the total number of features.\n",
    "\t- Implement a loop to build the specified number of decision trees:\n",
    "        - Generate a bootstrap sample from the training set (sampling with replacement).\n",
    "        - Build a decision tree using the bootstrap sample, using your implementation from Part I.\n",
    "        - Add the constructed decision tree to the Random Forest.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the Random Forest:\n",
    "\t- For each instance, pass it through all decision trees in the Random Forest and collect the predictions. Note that you should binarize the prediction of each decision tree, that is, use a threshold of 0.5 to determine the actual class label.\n",
    "\t- The prediction for the random forest will be the frequency of the positive class in the predictions collected by all the decision trees.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the function to learn the Random Forest with your training set. You will vary the different parameters of the Random Forest to observe their effect on the performance on the validation set. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Again, keep your trees shallow and don’t build many decision trees, as this could delay the training time quite a lot.\n",
    "\t- Build a Random Forest using the training + validation sets with the best combination of parameters.\n",
    "\t- Classify the instances of the testing set using the Random Forest, calculate the accuracy (threshold: 0.5) and Area Under the ROC Curve (AUROC) and report the results.\n",
    "5. Experimentation: Compare the performance of Random Forests with the single decision tree implementation from the previous exercise reporting the performance on the test set in a table (either a dataframe or markdown). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae13d9f-4ef0-4735-8190-1bcaaa28ea2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2950e57-8657-4066-b657-1c96c830ce6e",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "## Part 3 – Clustering with K-means (value: 40%)\n",
    "\n",
    "In this exercise, you will explore clustering by implementing the K-means algorithm. You will write code to perform K-means clustering while visualizing the movement of the centroids at each iteration. \n",
    "\n",
    "To complete this exercise, you will write code to implement K-means for clustering: \n",
    "\n",
    "1. Dataset Preparation: Run the cells provided in the notebook that generate the artificial data points for this exercise.\n",
    "2. K-means Clustering:\n",
    "\t- Initialize K cluster centroids by selecting K points from your dataset at random.\n",
    "\t- Implement a loop to perform the following steps until convergence (or until a specified maximum number of iterations is reached, e.g., 150):\n",
    "        - Assign each data point to the nearest centroid (you will have to calculate the Euclidean distance between the data point and each centroid).\n",
    "        - Update each centroid by moving it to the mean of all data points assigned to it.\n",
    "        - Check for convergence by comparing the new centroids with the previous centroids. If the difference is smaller than an $\\epsilon=1^{-4}$, exit the loop.\n",
    "3. Centroid Movement Visualization:\n",
    "\t- At 5 different moments during training, plot a figure showing the centroids and the points. Figure 1 should show the situation at the beginning, before learning. Figure 5 should show the situation at the end of the learning. The remaining Figures 2-4 should show intermediary situations.\n",
    "\t- For each figure, each centroid will be represented by a large black cross and each cluster with a different colour, the points must be coloured according to their respective cluster.\n",
    "4. Sum of squared distances:\n",
    "\t- Along with plotting the centroid movement, calculate the sum of squared distances at each iteration as follows:\n",
    "        - $\\sum_{j=1}^K \\sum_{n \\in S_j}d(x_n,\\mu_j )^2$, where $K$ is the number of clusters, $x_n$ represents the $n^{th}$ datapoint, $n \\in S_j$ indicates a set of points that belong to cluster $S_j$, $\\mu_j$ is the mean of the datapoints in $S_j$ and $d(x_n,\\mu_j)$ indicates the Euclidean distance between $x_n$ and $\\mu_j$.\n",
    "\t- Make a plot of the sum of squared distances at each iteration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03418cb-b1b2-4fd8-b0e9-6d1a21ae5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data points\n",
    "np.random.seed(13)\n",
    "num_samples = 200\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features) * 1.5 + np.array([[2, 2]])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 3 + np.array([[-5, -5]])])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 2 + np.array([[7, -5]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b782b9-0bd4-4085-a5f2-f55ca13208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
