{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a4c9d77-8ce0-4600-b5db-f98f39c8e9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from random import sample\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163fb6c-8186-42af-885f-8278e7ca2ce3",
   "metadata": {},
   "source": [
    "# Tarefa 4 - Decision Trees, Random Forest and K-Means\n",
    "Fourth assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using decision trees and random forests to solve a real-world problem in classification and delve into the world of unsupervised learning by implementing the K-means algorithm. Students will also get used to generating important plots during training to analyse the models' behaviour. \n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).\n",
    "* Please try to not use any LLM-generated code. This coursework is designed for you to learn crucial concepts. Once you master them, then using LLMs become much easier.\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Decision Trees](#Decision_Trees) (30%)\n",
    "2. [Random Forest](#Random_Forest) (30%)\n",
    "3. [K-Means](#K-Means) (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bb261-de78-4c09-bbff-f1128567f078",
   "metadata": {},
   "source": [
    "### Decision_Trees\n",
    "## Part 1 - Decision Trees for Classification (value: 30%)\n",
    "\n",
    "In this exercise, you will implement a decision tree for classifying whether the income of a person exceeds $50k/yr based on census data (adult_census_subset.csv in ECLASS). You will use the Information Gain based on the Gini Index as the impurity measure as the splitting criterion. The maximum depth and the minimum number of instances per leaf will be your stopping criteria. Be aware that some of the variables in this dataset are nominal (or categorical).\n",
    "\n",
    "To complete this exercise, you will write code to build a decision tree for this problem: \n",
    "\n",
    "1. Dataset Splitting:\n",
    "    - Load the provided dataset into your code.\n",
    "\t- Split the dataset into three sets: training, validation, and testing, with a 70/15/15 ratio, respectively. \n",
    "2. Implement a function to learn Decision Trees – the main conceptual steps are detailed below:\n",
    "\t- Initialize an empty decision tree.\n",
    "\t- Implement a recursive function to build the decision tree:\n",
    "        - The stopping conditions for the recursive function are [note: satisfying only one of them is sufficient to stop the recursion]:\n",
    "            - If the maximum depth is reached, stop growing the tree and create a leaf node with the frequency of the positive class for the remaining instances.\n",
    "            - If the number of instances at a leaf node is less than the minimum number of instances per leaf, create a leaf node with the frequency of the positive class for those instances.\n",
    "        - Your code will calculate the Information Gain (based on the Gini Index) for each possible value of each attribute and choose the attribute and value that maximizes the Information Gain (explanation below).\n",
    "        - Your code will create a new internal node using the chosen attribute and value.\n",
    "        - Your code will recursively call the build function on each subset of instances created by the split.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the decision tree:\n",
    "\t- For each instance, traverse the decision tree by comparing its attribute values to the decision nodes and move down the tree based on the attribute values until a leaf node is reached.\n",
    "\t- Return the frequency of the positive class that is associated with the leaf node as the prediction for the instance.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the build function with the training set to construct the decision tree. You will vary the maximum depth and minimum number of instances per leaf to observe their effects on the decision tree performance. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Try only shallow trees of a depth not deeper than 10, and min_instances not smaller than 10. If you try more extreme values, the training time could be too much.\n",
    "\t- Build a decision tree using the training + validation sets with the best combination of parameters.\n",
    "\t- Calculate the accuracy (threshold: 0.5) and AUROC of the decision tree in the testing set and report them.\n",
    "\n",
    "To select the best split at each node you will use the Information Gain based on the Gini Index. The Gini Index measures the impurity of a node in a decision tree. To calculate the Information Gain based on the Gini Index, follow these steps [note: the same is explained in the slides for the case of entropy]:\n",
    "- For each potential split (feature and value):\n",
    "\t- Calculate the Gini Index for node m (before any splits) using the class distribution within the node, using the following formula:\n",
    "        - $G_m=\\sum_{k=1}^K (\\hat{p}_{mk} (1-\\hat{p}_{mk})$, where $\\hat{p}_{mk}$ represents the proportion of instances in the node $m$ that belong to class $k$.\n",
    "\t- Calculate the Gini Index for each possible outcome. This involves the following steps:\n",
    "        - Split the data based on the attribute's possible outcomes.\n",
    "        - Calculate the Gini index for each resulting subset using the same formula as in step a.\n",
    "\t- Calculate the weighted Gini index by summing up the Gini indexes of each subset, weighted by the proportion of instances it represents in the original node. The formula for the weighted Gini index ($W$) is as follows:\n",
    "        - $W_V=\\sum_v^V \\frac{|S_v|}{|S|} G_{S_v} $ where $S_v$ is the node after the split and the sum iterates over all the children nodes; $|S_v|$ represents the cardinality of the node and $|S|$ the cardinality of the node before splitting; $G_{S_v}$ represents the Gini index of the node.\n",
    "\t- Calculate the information gain by subtracting the weighted Gini index obtained in step c. from the Gini index of the current node. The formula is as follows:\n",
    "        - $InformationGain=G_{node}-W_V$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "51329343-9ecb-4e89-8b7b-bdfd62c4d777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading my dataset\n",
    "adult_census = pd.read_csv(\"adult_census_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f353fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating dataset and target\n",
    "X = adult_census.drop(\"income\", axis=1)\n",
    "t = adult_census[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "25460fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_split, t_train, t_split = train_test_split(X, t, train_size=.7, random_state=0)\n",
    "X_validation, X_test, t_validation, t_test = train_test_split(X_split, t_split, train_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d0a4f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(x):\n",
    "    return np.issubdtype(type(x), np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ddf9e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=6, min_samples=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.root = None\n",
    "        # self.thresholds = []\n",
    "\n",
    "        # I create a list of threadholds that I'll update every\n",
    "        # terminal node I add\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, t: pd.Series):\n",
    "        \"\"\"This function fits my decision tree on the dataset and target passed\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            The dataset that will be fitted onto the tree\n",
    "        t : pd.Series\n",
    "            The target for reach datapoint of the passed dataset\n",
    "        \"\"\"\n",
    "        self.root = self.recursive_growth(0, X, t)\n",
    "    \n",
    "    def recursive_growth(self, current_depth: int, X: pd.DataFrame, t: pd.Series):\n",
    "        \"\"\"\n",
    "        Recursively grows a decision tree.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dict\n",
    "            If the node is terminal, it contains only the \"value\" property, which determines the value\n",
    "            to be used as a prediction.\n",
    "            If the node is not terminal, the Node has the structure defined by `get_split`\n",
    "        min_samples : int\n",
    "            parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "        max_depth : int\n",
    "            parameter for stopping criterion if a node belongs to this depth\n",
    "        depth : int\n",
    "            current distance from the root\n",
    "        X : pd.DataFrame\n",
    "            features (Of the actual node)\n",
    "        t : pd.Series\n",
    "            labels, targets (Of the actual node)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "        is the mean of the target variable\n",
    "        \n",
    "        'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "        from higher levels of the tree:\n",
    "        'left' corresponds to the 'left_region' key, and 'right' to the 'right_region' key\n",
    "        \"\"\"\n",
    "        splited_infos = self.get_split(X, t)\n",
    "        l_idx = splited_infos['left_region']\n",
    "        r_idx = splited_infos['right_region']\n",
    "\n",
    "        if                                                                                  \\\n",
    "            (len(l_idx) <= 0)                                                               \\\n",
    "            or                                                                              \\\n",
    "            (len(r_idx) <= 0)                                                               \\\n",
    "            or                                                                              \\\n",
    "            (splited_infos['information_gain'] == 0)                                        \\\n",
    "            or                                                                              \\\n",
    "            (len(X) <= self.min_samples)                                                    \\\n",
    "            or                                                                              \\\n",
    "            (current_depth >= self.max_depth)                                               \\\n",
    "        :\n",
    "            # Calculating the positive class' frequency\n",
    "            total = len(t)\n",
    "            class_1 = len(t[t==1])\n",
    "            value = class_1/total if total != 0 else None\n",
    "\n",
    "            # I update my threshold list based on the value I received. But the list\n",
    "            # must contain only unique values, so I check if the value's already in the list\n",
    "            # if value not in self.thresholds:\n",
    "            #     self.thresholds.append(value)\n",
    "\n",
    "            return {\n",
    "                \"value\": value\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'feature_index': splited_infos['feature_index'],\n",
    "            'tau': splited_infos['tau'],\n",
    "            'left': self.recursive_growth(current_depth+1, X.loc[l_idx], t.loc[l_idx]),\n",
    "            'right': self.recursive_growth(current_depth+1, X.loc[r_idx], t.loc[r_idx])\n",
    "        }\n",
    "    \n",
    "    def get_split(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Given a dataset (full or partial), splits it on the feature of that minimizes the\n",
    "        sum of squared error\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            features \n",
    "        y : pd.Series\n",
    "            labels\n",
    "        gini_index: float\n",
    "            This function is used \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        decision : dictionary\n",
    "            keys are:\n",
    "            * 'feature_index' -> an integer that indicates the feature (column) of `X`\n",
    "            on which the data is split\n",
    "            * 'tau' -> the threshold used to make the split\n",
    "            * 'left_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "            * 'right_region' -> indices not in `low_region`\n",
    "        \"\"\"\n",
    "        best_information_gain = -float(\"inf\")\n",
    "        best_tau = None\n",
    "        best_feature = None\n",
    "        # List containing the best tau and gini_index registered for each feature, follows this structure\n",
    "        gini_index = self.gini_index(y)\n",
    "        best_left_region = None\n",
    "        best_right_region = None\n",
    "\n",
    "        for feature in X.columns:  # Going through every feature\n",
    "            unique_features = pd.unique(X[feature])\n",
    "\n",
    "            for tau in unique_features:  # Using each value of the feature as tau (For performance reasons)\n",
    "                # First I get the node division based on the tau and the feature specified\n",
    "                l_feature, r_feature = self.split_region(X, feature, tau)\n",
    "                # I initialize my weighted gini index\n",
    "                tau_weighted_gini_index = 0\n",
    "\n",
    "                # I store the divisions on variables for organization purposes\n",
    "                y_right = y[r_feature]\n",
    "                y_left = y[l_feature]\n",
    "\n",
    "                # For organization purposes I store the amount of elements in the parent node, its\n",
    "                # left division and right division\n",
    "                amount_total = len(y)\n",
    "                amount_right = len(y_right)\n",
    "                amount_left = len(y_left)\n",
    "\n",
    "                if amount_right == 0 or amount_left == 0:\n",
    "                    tau_information_gain = 0\n",
    "                else:\n",
    "                    # I get the gini index for the left node and the right node\n",
    "                    gini_left = self.gini_index(y_left)\n",
    "                    gini_right = self.gini_index(y_right)\n",
    "\n",
    "                    # Now I sum up the weighted gini index as the definition says\n",
    "                    tau_weighted_gini_index += (amount_left/amount_total)*gini_left\n",
    "                    tau_weighted_gini_index += (amount_right/amount_total)*gini_right\n",
    "\n",
    "                    # Now I calculate the information gain for this specific feature and tau\n",
    "                    tau_information_gain = gini_index - tau_weighted_gini_index\n",
    "                    \n",
    "                if tau_information_gain > best_information_gain:\n",
    "                    best_information_gain = tau_information_gain\n",
    "                    best_tau = tau\n",
    "                    best_feature = feature\n",
    "                    best_left_region = l_feature\n",
    "                    best_right_region = r_feature\n",
    "\n",
    "        return {\n",
    "            'feature_index': best_feature,\n",
    "            'tau': best_tau,\n",
    "            'left_region': best_left_region,\n",
    "            'right_region': best_right_region,\n",
    "            'information_gain': best_information_gain\n",
    "        }\n",
    "    \n",
    "    def split_region(self, region: pd.Series | pd.DataFrame, feature_index: int|str, tau: float):\n",
    "        \"\"\"\n",
    "        Given a region, splits it based on the feature indicated by\n",
    "        `feature_index`, the region will be split in two, where\n",
    "        one side will contain all points with the feature with values \n",
    "        lower than `tau`, and the other split will contain the \n",
    "        remaining datapoints.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : pd.Series | pd.DataFrame\n",
    "            a partition of the dataset (or the full dataset) to be split\n",
    "        feature_index : str | int\n",
    "            the index of the feature (column of the region array) used to make this partition\n",
    "        tau : float\n",
    "            The threshold used to make this partition\n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        left_partition : pd.Series | pd.DataFrame\n",
    "            indices of the datapoints in `region` where feature < `tau`\n",
    "        right_partition : pd.Series | pd.DataFrame\n",
    "            indices of the datapoints in `region` where feature >= `tau` \n",
    "        \"\"\"\n",
    "        if is_number(tau):\n",
    "            left_partition = region[region[feature_index] < tau][feature_index].index\n",
    "            right_partition = region[region[feature_index] >= tau][feature_index].index\n",
    "        else:\n",
    "            left_partition = region[region[feature_index] != tau][feature_index].index\n",
    "            right_partition = region[region[feature_index] == tau][feature_index].index\n",
    "\n",
    "        return left_partition, right_partition\n",
    "    \n",
    "    def gini_index(self, region: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Implements the gini index criterion in a region\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : pd.Series\n",
    "            Array of shape (N,) containing the values of the target values \n",
    "            for N datapoints in the training set.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The gini index of the funcion\n",
    "            \n",
    "        Note\n",
    "        ----\n",
    "        The error for an empty region should be 0 in order to be able to maximize\n",
    "        the information gain\n",
    "        \"\"\"\n",
    "        if len(region) == 0:\n",
    "            return -float(\"inf\")\n",
    "        if isinstance(region, pd.Series):\n",
    "            region = region.to_numpy()\n",
    "        \n",
    "        classes = np.unique(region)\n",
    "        Gm = 0\n",
    "        \n",
    "        for k in classes:\n",
    "            p_mk = len(region[region == k]) / len(region)\n",
    "            Gm += p_mk * (1 - p_mk)\n",
    "\n",
    "        return Gm\n",
    "    \n",
    "    def predict_sample(self, sample: pd.Series, node: dict):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : dictionary\n",
    "            A node created one of the methods above\n",
    "        sample : array of size (n_features,)\n",
    "            a sample datapoint\n",
    "        threshold : float\n",
    "            The threshold that indicates when its from positive class (Highter than\n",
    "            the threshold indicates the prediction is from class 1)\n",
    "        \"\"\"\n",
    "        if 'value' in node.keys():  # If its a value node\n",
    "            return node['value']\n",
    "        else:\n",
    "            # Checking if a empty node exists, if that's so, go to the other node\n",
    "            if 'value' in node['right'].keys() and node['right']['value'] is None:\n",
    "                return self.predict_sample(sample, node['left'])\n",
    "            elif 'value' in node['left'].keys() and node['left']['value'] is None:\n",
    "                return self.predict_sample(sample, node['right'])\n",
    "            \n",
    "            if is_number(node['tau']):  # If the threshold is numeric\n",
    "                if sample[node['feature_index']] <= node['tau']:  # Compare if the sample value is greater or lower\n",
    "                    return self.predict_sample(sample, node['left'])\n",
    "                elif sample[node['feature_index']] > node['tau']:\n",
    "                    return self.predict_sample(sample, node['right'])\n",
    "            else:  # If the threshold is nominal\n",
    "                if sample[node['feature_index']] != node['tau']:  # Check if the sample value is equal or different\n",
    "                    return self.predict_sample(sample, node['left'])\n",
    "                elif sample[node['feature_index']] == node['tau']:\n",
    "                    return self.predict_sample(sample, node['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of size (n_samples, n_features)\n",
    "            n_samples predictions will be made\n",
    "        \"\"\"\n",
    "        predicted_values = pd.Series(np.array([x for x in range(len(X))]))\n",
    "        predicted_values = predicted_values.apply(\n",
    "            lambda i: self.predict_sample(\n",
    "                X.iloc[i],\n",
    "                self.root,\n",
    "            )\n",
    "        )\n",
    "        return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a7a39476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measures:\n",
    "    @classmethod\n",
    "    def get_measures(cls, y_prediction: np.ndarray | pd.Series, t_target: np.ndarray | pd.Series) -> dict:\n",
    "        \"\"\"This function returns the 4 main measures of classification analysis:\n",
    "        - TRUE POSITIVES\n",
    "        - TRUE NEGATIVES\n",
    "        - FALSE POSITIVES\n",
    "        - FALSE NEGATIVES\n",
    "\n",
    "        Args:\n",
    "            y_prediction (np.ndarray | pd.Series): The predicted values\n",
    "            t_target (np.ndarray | pd.Series): The target values (True labels)\n",
    "        \n",
    "        Returns:\n",
    "            (dict): A dictionary with each measure, the format has the following structure:\n",
    "            ```\n",
    "            {\n",
    "                \"TP\": float,\n",
    "                \"TN\": float,\n",
    "                \"FP\": float,\n",
    "                \"FN\": float,\n",
    "            }\n",
    "            ```\n",
    "        \"\"\"\n",
    "        TP = len(y_prediction[(y_prediction == 1)&(t_target == 1)])\n",
    "        FP = len(y_prediction[(y_prediction == 1)&(t_target == 0)])\n",
    "        TN = len(y_prediction[(y_prediction == 0)&(t_target == 0)])\n",
    "        FN = len(y_prediction[(y_prediction == 0)&(t_target == 1)])\n",
    "\n",
    "        return {\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'TN': TN,\n",
    "            'FN': FN\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def accuracy(cls, y_prediction: np.ndarray | pd.Series, t_target: np.ndarray | pd.Series) -> float:\n",
    "        \"\"\"Calculates a prediction's accuracy\n",
    "\n",
    "        Args:\n",
    "            y_prediction (np.ndarray | pd.Series): Predicted values\n",
    "            t_target (np.ndarray | pd.Series): Target values (True labels)\n",
    "\n",
    "        Returns:\n",
    "            float: The prediction's accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        measures = cls.get_measures(y_prediction, t_target)\n",
    "\n",
    "        return (\n",
    "            measures['TP'] + measures['TN']\n",
    "        )/(\n",
    "            measures['TP'] + measures['TN'] + measures['FP'] + measures['FN']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5b5fd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the validations\n",
    "max_depths = [x+1 for x in range(11)]\n",
    "min_instances = [x for x in range(10, 61, 5)]\n",
    "\n",
    "trees = []\n",
    "\n",
    "if not os.path.exists(\"decision_tree_results.csv\"):\n",
    "    for max_depth in max_depths:\n",
    "        for min_instance in min_instances:\n",
    "            tree = DecisionTree(max_depth=max_depth, min_samples=min_instance)\n",
    "            tree.fit(X_train, t_train)\n",
    "\n",
    "            trees.append((max_depth, min_instance, tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eeada9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"decision_tree_results.csv\"):\n",
    "    validation_results = []  # List that will store my validation results for later analysis\n",
    "\n",
    "    for depth, instance, tree in trees:\n",
    "        y_validation = tree.predict(X_validation)\n",
    "        \n",
    "        calculated_score = roc_auc_score(t_validation, y_validation)\n",
    "\n",
    "        validation_results.append((calculated_score, depth, instance))\n",
    "\n",
    "    results_to_save = pd.DataFrame(validation_results, columns=[\"SCORE\", \"MAX_DEPTH\", \"MIN_SAMPLES_PER_LEAF\"])\n",
    "    results_to_save.to_csv(\"decision_tree_results.csv\", index=False)\n",
    "else:\n",
    "    validation_results = pd.read_csv(\"decision_tree_results.csv\")\n",
    "    validation_results = validation_results.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2f42979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = max(validation_results, key=lambda r: r[0])\n",
    "\n",
    "max_depth = best_results[1]\n",
    "min_instance = best_results[2]\n",
    "\n",
    "X_training = pd.concat([X_train, X_validation])\n",
    "t_training = pd.concat([t_train, t_validation])\n",
    "\n",
    "tree = DecisionTree(max_depth=max_depth, min_samples=min_instance)\n",
    "\n",
    "tree.fit(X_training, t_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e9a07fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Melhor Área ROC (Validação): 0.8329678362573099\n",
      "==============================\n",
      "Área ROC (Teste): 0.8596491228070176\n",
      "Acurácia (Teste): 0.6875\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "y_test = tree.predict(X_validation)\n",
    "\n",
    "roc = roc_auc_score(t_validation, y_test)\n",
    "accuracy = Measures.accuracy(y_test>=0.5, t_validation)\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(f\"Melhor Área ROC (Validação): {best_results[0]}\")\n",
    "print(f\"=\"*30)\n",
    "print(f\"Área ROC (Teste): {roc}\")\n",
    "print(f\"Acurácia (Teste): {accuracy}\")\n",
    "print(f\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "768de856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth):\n",
    "    if 'value' in node.keys():\n",
    "        print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "    else:\n",
    "        if is_number(node['tau']):\n",
    "            print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "        else:\n",
    "            print('.  '*depth, f'X_{node[\"feature_index\"]} != {node[\"tau\"]}')\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "148111bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_marital.status != Married-civ-spouse\n",
      ".   X_capital.gain < 13550\n",
      ".  .   X_fnlwgt < 635913\n",
      ".  .  .   X_education != Doctorate\n",
      ".  .  .  .   X_native.country != Italy\n",
      ".  .  .  .   [0.04044117647058824]\n",
      ".  .  .  .   [1.0]\n",
      ".  .  .   [1.0]\n",
      ".  .   [1.0]\n",
      ".   [1.0]\n",
      ".   X_education.num < 13\n",
      ".  .   X_capital.gain < 5178\n",
      ".  .  .   X_education.num < 9\n",
      ".  .  .   [0.045454545454545456]\n",
      ".  .  .  .   X_age < 44\n",
      ".  .  .  .   [0.22580645161290322]\n",
      ".  .  .  .   [0.4264705882352941]\n",
      ".  .   [1.0]\n",
      ".  .   X_age < 76\n",
      ".  .  .   X_age < 33\n",
      ".  .  .   [0.3333333333333333]\n",
      ".  .  .  .   X_age < 56\n",
      ".  .  .  .   [0.8214285714285714]\n",
      ".  .  .  .   [0.5]\n",
      ".  .   [0.0]\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree.root, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a1622-54da-4aa8-a1c2-00f3241292be",
   "metadata": {},
   "source": [
    "## Random_Forest\n",
    "## Part 2 - Random Forest for Classification Networks (value: 30%)\n",
    "\n",
    "In this exercise, you will expand on the previous exercise and implement Random Forests. You will build an ensemble of decision trees and use them for the same classification task from Part 1. The dataset used for this exercise will be the same as in the previous exercise. Your task is to write code to construct a Random Forest model, evaluate its performance, and compare it to the decision tree implementation. \n",
    "\n",
    "To complete this exercise, you will write code to implement Random Forest for this problem: \n",
    "\n",
    "1. Dataset Splitting: use the same splits you used for Part 1.\n",
    "2. Implement a function to learn Random Forest – the main steps are detailed below:\n",
    "\t- Initialize an empty Random Forest.\n",
    "\t- Determine the number of decision trees to include in the forest (e.g. 20), and the number of the random features to consider, generally `num_features` $≈\\sqrt{p}$ where $p$ is the total number of features.\n",
    "\t- Implement a loop to build the specified number of decision trees:\n",
    "        - Generate a bootstrap sample from the training set (sampling with replacement).\n",
    "        - Build a decision tree using the bootstrap sample, using your implementation from Part I.\n",
    "        - Add the constructed decision tree to the Random Forest.\n",
    "3. Implement a classification function. Implement a function to classify new instances using the Random Forest:\n",
    "\t- For each instance, pass it through all decision trees in the Random Forest and collect the predictions. Note that you should binarize the prediction of each decision tree, that is, use a threshold of 0.5 to determine the actual class label.\n",
    "\t- The prediction for the random forest will be the frequency of the positive class in the predictions collected by all the decision trees.\n",
    "4. Run your algorithm and evaluate its performance:\n",
    "\t- Call the function to learn the Random Forest with your training set. You will vary the different parameters of the Random Forest to observe their effect on the performance on the validation set. You will use the training set to learn the tree and the validation set using the Area Under the Roc Curve (AUROC) to find the optimal parameters. Again, keep your trees shallow and don’t build many decision trees, as this could delay the training time quite a lot.\n",
    "\t- Build a Random Forest using the training + validation sets with the best combination of parameters.\n",
    "\t- Classify the instances of the testing set using the Random Forest, calculate the accuracy (threshold: 0.5) and Area Under the ROC Curve (AUROC) and report the results.\n",
    "5. Experimentation: Compare the performance of Random Forests with the single decision tree implementation from the previous exercise reporting the performance on the test set in a table (either a dataframe or markdown). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "33240ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading my dataset\n",
    "adult_census = pd.read_csv(\"adult_census_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28b1b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating dataset and target\n",
    "X = adult_census.drop(\"income\", axis=1)\n",
    "t = adult_census[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d9544eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_split, t_train, t_split = train_test_split(X, t, train_size=.7, random_state=0)\n",
    "X_validation, X_test, t_validation, t_test = train_test_split(X_split, t_split, train_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "01b42ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, number_of_trees: int, max_depth: int, min_samples_per_leaf: int):\n",
    "        self.number_of_trees: int = number_of_trees\n",
    "        self.trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_per_leaf = min_samples_per_leaf\n",
    "    \n",
    "    @staticmethod\n",
    "    def bootstrap(X:np.ndarray|pd.DataFrame, num_bags:int=10)->np.ndarray:\n",
    "        \"\"\"Given a dataset and a number of bags, sample the dataset with replacement.\n",
    "        \n",
    "        This function does not return a copy of the datapoints, but a list of indexes\n",
    "        with compatible dimensionality\n",
    "        \n",
    "        Notes:\n",
    "            - The number of datapoints in each bach will\n",
    "            match the number of datapoints in the given\n",
    "            dataset.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray | pd.DataFrame): The dataset\n",
    "            num_bags (int, optional): Number of bags to create. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "            Each of these contains the indices corresponding to the sampled datapoints in `X`\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "\n",
    "        bags = []\n",
    "        \n",
    "        for _ in range(num_bags):\n",
    "            bags.append(np.random.randint(0, n, n))\n",
    "        \n",
    "        return np.array(bags)\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, t:pd.Series, num_features:int=None):\n",
    "        \"\"\"Fits the random forest to the passed dataset and target array. It uses the bagging\n",
    "        method for the random forest\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Dataset with shape `(n_samples, n_features)`\n",
    "            t (pd.Series): Target variables with shape `(n_samples,)`\n",
    "            num_features (int): Amount of random features that will be used in each bag.\n",
    "            If set to `None`, the number of features will be `√p`, where `p` is the total\n",
    "            number of features in the dataset\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "\n",
    "        features = X.columns.to_list()  # I get all features I have on my dataset\n",
    "\n",
    "        bags = self.bootstrap(X, self.number_of_trees)  # I create all the bootstraps samples\n",
    "\n",
    "        # I get the amount of features I'll have per bootstraped dataset\n",
    "        amount_of_features = round(np.sqrt(X.shape[1])) if num_features is None else num_features\n",
    "\n",
    "        for bag in bags:  # For each index sequence\n",
    "            bag_features = sample(features, amount_of_features)\n",
    "            X_bag = X.iloc[bag][bag_features].reset_index(drop=True)\n",
    "            t_bag = t.iloc[bag].reset_index(drop=True)\n",
    "\n",
    "            tree_bag = DecisionTree(self.max_depth, self.min_samples_per_leaf)\n",
    "            tree_bag.fit(X_bag, t_bag)\n",
    "\n",
    "            self.trees.append(tree_bag)\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"Makes the prediction of a Dataset based on the trees generated by the `fit` method\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): The data that will be predicted\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for tree in self.trees:\n",
    "            pred = tree.predict(X)\n",
    "\n",
    "            predictions.append((pred >= 0.5).apply(lambda i: int(i)))\n",
    "        \n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e4f91e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "# Making the validation, I'll use multiprocessing for a better execution time\n",
    "\n",
    "\n",
    "def create_and_predict_random_forest(\n",
    "    analysis_list,\n",
    "    i,\n",
    "    number_of_trees,\n",
    "    max_depth,\n",
    "    min_samples_per_leaf,\n",
    "):\n",
    "    analysis_list[i][1] = number_of_trees\n",
    "    analysis_list[i][2] = max_depth\n",
    "    analysis_list[i][3] = min_samples_per_leaf\n",
    "\n",
    "    forest = RandomForest(\n",
    "        number_of_trees=number_of_trees,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_per_leaf=min_samples_per_leaf\n",
    "    )\n",
    "\n",
    "    forest.fit(X_train, t_train)\n",
    "\n",
    "    y_validation = forest.predict(X_validation)\n",
    "\n",
    "    analysis_list[i][0] = roc_auc_score(t_validation, y_validation)\n",
    "    \n",
    "\n",
    "\n",
    "max_depths = [x+1 for x in range(11)]\n",
    "mins_samples_per_leafs = [x for x in range(10, 61, 5)]\n",
    "numbers_of_trees = [x for x in range(6, 16, 3)]\n",
    "\n",
    "\n",
    "values_for_test = []  # List that have the args for the create_and_predict_random_forest\n",
    "# The elements follows this structure: (number_of_trees, max_depth, min_samples_per_leaf)\n",
    "\n",
    "for number_of_trees in numbers_of_trees:\n",
    "    for max_depth in max_depths:\n",
    "        for min_samples_per_leaf in mins_samples_per_leafs:\n",
    "            values_for_test.append((\n",
    "                number_of_trees,\n",
    "                max_depth,\n",
    "                min_samples_per_leaf,\n",
    "            ))\n",
    "    \n",
    "\n",
    "if not os.path.exists(\"random_forest_results.csv\"):\n",
    "    with mp.Manager() as manager:\n",
    "        analysis_list = manager.list([manager.list([0, 0, 0, 0]) for _ in range(len(values_for_test))])\n",
    "        # AUC ROC, Number of Trees, Max Depth, Min Samples per Leaf\n",
    "        processes: list[mp.Process] = []\n",
    "\n",
    "        for i, (number_of_trees, max_depth, min_samples_per_leaf) in enumerate(values_for_test):\n",
    "            p = mp.Process(\n",
    "                target=create_and_predict_random_forest,\n",
    "                args=(\n",
    "                    analysis_list,\n",
    "                    i,\n",
    "                    number_of_trees,\n",
    "                    max_depth,\n",
    "                    min_samples_per_leaf,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            p.start()\n",
    "\n",
    "            processes.append(p)\n",
    "        \n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        final_data = [list(row) for row in analysis_list]\n",
    "\n",
    "        df = pd.DataFrame(final_data, columns=[\n",
    "            \"AUC ROC\", \"Number of Trees\", \"Max Depth\", \"Min Samples per Leaf\"\n",
    "        ])\n",
    "        df.to_csv(\"random_forest_results.csv\", index=False)\n",
    "else:\n",
    "    final_data = pd.read_csv(\"random_forest_results.csv\")\n",
    "    final_data = final_data.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "735c1e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8954678362573099, 6.0, 3.0, 25.0]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results = max(final_data, key=lambda result: result[0])\n",
    "\n",
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "edc3b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trees = int(best_results[1])\n",
    "max_depth = int(best_results[2])\n",
    "min_samples_per_leaf = int(best_results[3])\n",
    "\n",
    "forest = RandomForest(number_of_trees, max_depth, min_samples_per_leaf)\n",
    "\n",
    "X_final = pd.concat([X_train, X_validation])\n",
    "t_final = pd.concat([t_train, t_validation])\n",
    "\n",
    "forest.fit(X_final, t_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e5086af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC Score: 0.6903935185185185\n",
      "Test Accuracy: 0.7472527472527473\n"
     ]
    }
   ],
   "source": [
    "y_test = forest.predict(X_test)\n",
    "\n",
    "roc = roc_auc_score(t_test, y_test)\n",
    "accuracy = Measures.accuracy(y_test>=.5, t_test)\n",
    "\n",
    "print(f\"Test ROC Score: {roc}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2950e57-8657-4066-b657-1c96c830ce6e",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "## Part 3 – Clustering with K-means (value: 40%)\n",
    "\n",
    "In this exercise, you will explore clustering by implementing the K-means algorithm. You will write code to perform K-means clustering while visualizing the movement of the centroids at each iteration. \n",
    "\n",
    "To complete this exercise, you will write code to implement K-means for clustering: \n",
    "\n",
    "1. Dataset Preparation: Run the cells provided in the notebook that generate the artificial data points for this exercise.\n",
    "2. K-means Clustering:\n",
    "\t- Initialize K cluster centroids by selecting K points from your dataset at random.\n",
    "\t- Implement a loop to perform the following steps until convergence (or until a specified maximum number of iterations is reached, e.g., 150):\n",
    "        - Assign each data point to the nearest centroid (you will have to calculate the Euclidean distance between the data point and each centroid).\n",
    "        - Update each centroid by moving it to the mean of all data points assigned to it.\n",
    "        - Check for convergence by comparing the new centroids with the previous centroids. If the difference is smaller than an $\\epsilon=1^{-4}$, exit the loop.\n",
    "3. Centroid Movement Visualization:\n",
    "\t- At 5 different moments during training, plot a figure showing the centroids and the points. Figure 1 should show the situation at the beginning, before learning. Figure 5 should show the situation at the end of the learning. The remaining Figures 2-4 should show intermediary situations.\n",
    "\t- For each figure, each centroid will be represented by a large black cross and each cluster with a different colour, the points must be coloured according to their respective cluster.\n",
    "4. Sum of squared distances:\n",
    "\t- Along with plotting the centroid movement, calculate the sum of squared distances at each iteration as follows:\n",
    "        - $\\sum_{j=1}^K \\sum_{n \\in S_j}d(x_n,\\mu_j )^2$, where $K$ is the number of clusters, $x_n$ represents the $n^{th}$ datapoint, $n \\in S_j$ indicates a set of points that belong to cluster $S_j$, $\\mu_j$ is the mean of the datapoints in $S_j$ and $d(x_n,\\mu_j)$ indicates the Euclidean distance between $x_n$ and $\\mu_j$.\n",
    "\t- Make a plot of the sum of squared distances at each iteration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a03418cb-b1b2-4fd8-b0e9-6d1a21ae5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data points\n",
    "np.random.seed(13)\n",
    "num_samples = 200\n",
    "num_features = 2\n",
    "X = np.random.randn(num_samples, num_features) * 1.5 + np.array([[2, 2]])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 3 + np.array([[-5, -5]])])\n",
    "X = np.concatenate([X, np.random.randn(num_samples, num_features) * 2 + np.array([[7, -5]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "47b782b9-0bd4-4085-a5f2-f55ca13208eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
